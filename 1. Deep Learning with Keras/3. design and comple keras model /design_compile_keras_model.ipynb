{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For this model, I am using imdb data from Keras datasets. Basically, this dataset has 25,000 movie reviews from IMDB, labeled by sentiment(either positive or negative). The reviews have been preprossed, each review is encoded as a sequence of word indexes which are (integers). Words are indexed by overall frequency in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras \n",
    "import numpy as np \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, BatchNormalization \n",
    "from keras.layers import LSTM, Embedding, Input, merge, Bidirectional\n",
    "from keras.preprocessing import sequence\n",
    "from keras import optimizers\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpecifyParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 2000\n",
    "max_len = 200\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "n_classes = 2\n",
    "\n",
    "embedding_dim = 128\n",
    "lstm_layer_dim = 64\n",
    "n_val_samples = 5000\n",
    "learning_rate = 0.01\n",
    "decay = learning_rate / epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load the IMDB dataset. We are constraining the dataset to the top 2,000 words. We also split the dataset into training, testing and validation sets.\n",
    "\n",
    "Notice that the data has been already pre-processed, where all the words have numbers, and the reviews come in as a vector with the words that the review contains. \n",
    "\n",
    "The output comes as a vector of 1's and 0's, where 1 is a positive sentiment for the review, and 0 is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test,y_test) = imdb.load_data(num_words = max_features)\n",
    "x_train = x_train[:-n_val_samples]\n",
    "y_train = y_train[:-n_val_samples]\n",
    "x_val = x_train[-n_val_samples:]\n",
    "y_val = y_train[-n_val_samples:]\n",
    "print('x_train Shape: ', x_train.shape)\n",
    "print('y_train Shape: ', y_train.shape)\n",
    "print('x_val Shape: ', x_val.shape)\n",
    "print('y_val Shape: ', y_val.shape)\n",
    "print('x_test Shape: ', x_test.shape)\n",
    "print('y_test Shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll turn the input vectors into (0,1)-vectors. For example, if the pre-processed vector contains the number 14, then in the processed vector, the 14th entry will be 1. \n",
    "\n",
    "\n",
    "Also, we need to truncate and pad the input sequences so that they are all the same length for modeling. The model will learn the zero values carry no information so indeed the sequences are not the same length in terms of content, but same length vectors is required to perform the computation in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen= max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen = max_len)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen= max_len)\n",
    "\n",
    "print('After Padding x_train Shape: ', x_train.shape)\n",
    "print('After padding x_test Shape: ', x_test.shape)\n",
    "print('After padding x_val Shape: ', x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll also one-hot encode the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, n_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, n_classes)\n",
    "print('Training set labels size: ' , y_train.shape)\n",
    "print('Test set labels size: ', y_test.shape)\n",
    "print('Evaluating set label size: ',y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer is the Embedded layer that uses 128 length vectors to represent each word. Then we add batch normalization layer here to normalize the value after embedding for the next layer. The next layer is the Bi-directional LSTM layer with 64 memory units. Then we add a dropout layer to reduce overfitting. \n",
    "\n",
    "Finally, because this is a classification problem we use a Dense output layer with a single neuron and a softmax activation function to produce the probability of each label('positive' or 'negative') for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option1: Sequential Model \n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim, input_length = max_len))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Bidirectional(LSTM(lstm_layer_dim)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option2: Functional API \n",
    "sequence = Input(shape = (max_len, ), dtype = np.int32)\n",
    "embedding = Embedding(max_features, embedding_dim, input_length = max_len)(sequence)\n",
    "batch_norm = BatchNormalization()(embedding)\n",
    "\n",
    "bi_lstm = Bidirectional(LSTM(lstm_layer_dim))(batch_norm)\n",
    "drop_out = Dropout(0.25)(bi_lstm)\n",
    "preds = Dense(2,activation='softmax')(drop_out)\n",
    "model = Model(sequence, preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Compilation\n",
    "Compile the model here. Feel free to experiment with different optimizers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Compile the model using a loss function and an optimizer.\n",
    "sgd = optimizers.SGD(lr = learning_rate, decay = decay, momentum= 0.9, nesterov= True)\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = sgd, \n",
    "              metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
