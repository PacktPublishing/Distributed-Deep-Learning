{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "import numpy as np \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, BatchNormalization \n",
    "from keras.layers import LSTM, Embedding, Input, merge, Bidirectional\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD\n",
    "import time\n",
    "import os\n",
    "\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 2000\n",
    "max_len = 200\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "n_classes = 2\n",
    "embedding_dim = 256\n",
    "lstm_layer_dim = 64\n",
    "n_val_samples = 5000\n",
    "learning_rate = 0.01\n",
    "decay = learning_rate / epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load the IMDB dataset. We are constraining the dataset to the top 2,000 words. We also split the dataset into train (50%) and test (50%) sets.\n",
    "\n",
    "Notice that the data has been already pre-processed, where all the words have numbers, and the reviews come in as a vector with the words that the review contains. \n",
    "\n",
    "The output comes as a vector of 1's and 0's, where 1 is a positive sentiment for the review, and 0 is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test,y_test) = imdb.load_data(num_words = max_features)\n",
    "x_train = x_train[:-n_val_samples]\n",
    "y_train = y_train[:-n_val_samples]\n",
    "\n",
    "print('x_train Shape: ', x_train.shape)\n",
    "print('y_train Shape: ', y_train.shape)\n",
    "print('x_test Shape: ', x_test.shape)\n",
    "print('y_test Shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll turn the input vectors into (0,1)-vectors. For example, if the pre-processed vector contains the number 14, then in the processed vector, the 14th entry will be 1. \n",
    "\n",
    "\n",
    "Also, we need to truncate and pad the input sequences so that they are all the same length for modeling. The model will learn the zero values carry no information so indeed the sequences are not the same length in terms of content, but same length vectors is required to perform the computation in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen= max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen = max_len)\n",
    "\n",
    "print('After Padding x_train Shape: ', x_train.shape)\n",
    "print('After padding x_test Shape: ', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll also one-hot encode the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, n_classes)\n",
    "\n",
    "print('Training set labels size: ' , y_train.shape)\n",
    "print('Test set labels size: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer is the Embedded layer that uses 128 length vectors to represent each word. Then we add batch normalization layer here to normalize the value after embedding for the next layer. The next layer is the Bi-directional LSTM layer with 64 memory units. Then we add a dropout layer to reduce overfitting. \n",
    "\n",
    "Finally, because this is a classification problem we use a Dense output layer with a single neuron and a softmax activation function to produce the probability of each label('positive' or 'negative') for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option1: Sequential Model \n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim, input_length = max_len))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Bidirectional(LSTM(lstm_layer_dim)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option2: Functional API \n",
    "sequence = Input(shape = (max_len, ), dtype = np.int32)\n",
    "embedding = Embedding(max_features, embedding_dim, input_length = max_len)(sequence)\n",
    "batch_norm = BatchNormalization()(embedding)\n",
    "\n",
    "bi_lstm = Bidirectional(LSTM(lstm_layer_dim))(batch_norm)\n",
    "drop_out = Dropout(0.25)(bi_lstm)\n",
    "preds = Dense(2,activation='softmax')(drop_out)\n",
    "\n",
    "model = Model(sequence, preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Compilation\n",
    "Compile the model here. Feel free to experiment with different optimizers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model using a loss function and an optimizer.\n",
    "sgd = SGD(lr = learning_rate, decay = decay, momentum= 0.9, nesterov= True)\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = sgd, \n",
    "              metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Run the model here. Feel free to experiment with different batch_size, and number of epochs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Train Model \n",
    "model.fit(x_train,y_train, \n",
    "         epochs= epochs,\n",
    "         batch_size = batch_size,\n",
    "         verbose = 1,\n",
    "         validation_split=0.2,\n",
    "         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 198s 12ms/step - loss: 0.4600 - acc: 0.7869 - val_loss: 0.4099 - val_acc: 0.8123\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.81225, saving model to imdb.model_best.hdf5\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 198s 12ms/step - loss: 0.3884 - acc: 0.8305 - val_loss: 0.3784 - val_acc: 0.8343\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.81225 to 0.83425, saving model to imdb.model_best.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb755bd2860>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Optional\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "checkpoint = ModelCheckpoint(filepath = 'imdb.model_best.hdf5',\n",
    "                             verbose = 1, \n",
    "                             monitor = 'val_acc',\n",
    "                            save_best_only = True)\n",
    "earlystopping = EarlyStopping(monitor ='val_acc', min_delta=0)\n",
    "\n",
    "model.fit(x_train,y_train, \n",
    "         epochs= epochs,\n",
    "         batch_size = batch_size,\n",
    "         verbose = 1,\n",
    "         validation_split=0.2,\n",
    "         shuffle=True,\n",
    "         callbacks =[checkpoint, earlystopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you set **verbose** to 1, you will be able to see the log line printed under after every batch.. \n",
    "\n",
    "After creating the **checkpoint**, you pass it as a parameter when you fit the model.\n",
    "\n",
    "When we set 0.2 as **validation_split ratio**, the function will set 20% of training data as validation set.\n",
    "\n",
    "By setting **save_best_only** parameter as True you can tell the model to save only the weights to get the best accuracy to the validation set.\n",
    "\n",
    "**ModelCheckpoint**: it will save the model after each epoch.\n",
    "* $filepath$: string, path to save the model file.\n",
    "monitor: quantity to monitor.\n",
    "* $verbose$: verbosity mode, 0 or 1.\n",
    "* save_best_only: if save_best_only=True, the latest best model according to the quantity monitored will not be overwritten.\n",
    "* $mode$: one of {auto, min, max}. If save_best_only=True, the decision to overwrite the current save file is made based on either the maximization or the minimization of the monitored quantity. For val_acc, this should be max, for  val_loss this should be min, etc. In auto mode, the direction is automatically inferred from the name of the monitored quantity.\n",
    "\n",
    "**EarlyStopping**: it will stop training when a monitored quantity has stopped improving.\n",
    "* $monitor$: quantity to be monitored.\n",
    "* min_delta: minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement.\n",
    "* $patience$: number of epochs with no improvement after which training will be stopped.\n",
    "* $verbose$: verbosity mode.\n",
    "* $mode$: one of {auto, min, max}. In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "This will give you the accuracy and scores of the model, as evaluated on the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 60s 2ms/step\n",
      "Test Accuracy: 83.84%\n"
     ]
    }
   ],
   "source": [
    "#evaluate test accuracy\n",
    "socres,acc = model.evaluate(x_test,y_test,\n",
    "                            batch_size = batch_size,\n",
    "                            verbose=1)\n",
    "print('Test Accuracy: %.2f%%' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions \n",
    "predictions = model.predict(x_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictons for the first 10 test samples: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.383759  , 0.61624104],\n",
       "       [0.11922486, 0.88077515],\n",
       "       [0.7489442 , 0.25105575],\n",
       "       [0.59171015, 0.40828985],\n",
       "       [0.00688119, 0.99311876],\n",
       "       [0.22190663, 0.77809334],\n",
       "       [0.4930913 , 0.5069087 ],\n",
       "       [0.95532894, 0.04467105],\n",
       "       [0.25497398, 0.745026  ],\n",
       "       [0.17399378, 0.82600623]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Predictons for the first 10 test samples: ')\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
